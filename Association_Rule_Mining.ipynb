{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Association_Rule_Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVEVQAf51pjv",
        "colab_type": "text"
      },
      "source": [
        "Steps to start with colab:\n",
        "* Click on the CONNECT button on right side of the notebook above.\n",
        "* Select \"Connect to host runtime\".\n",
        "* Once connected you will see RAM and Disk alloted.\n",
        "* CSVs are available in Sample Data Folder which can be accessed using the Folder symbol on the left panel.\n",
        "* Rest of the commands etc are same as jupyter notebook.\n",
        "\n",
        "***Note: In case unable to connect to hosted runtime configure local runtime. Ref: https://medium.com/@jasonrichards911/getting-local-with-google-colab-a4d69f373364***\n",
        "\n",
        "***Note: The RAM and Disk allocated after connecting to hosted runtime are upgraded automatically once the limit is reached. You will be prompted with notification just accept that and continue with you work.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_sS3bChnvbO",
        "colab_type": "text"
      },
      "source": [
        "Attributes for ARM: {state, rural/urban, sex, age, weight, height, iodine salt test, haemoglobin level, systolic BP, diastolic BP, pulse rate, fasting glucose level}\n",
        "\n",
        "Categorical: {state, rural/urban, sex}\n",
        "Continuous: {age, BMI, iodine salt test, haemoglobin level, systolic BP, diastolic BP, pulse rate, fasting glucose level}\n",
        "\n",
        "* Haemoglobin levels: CONCLUSION DEPENDS ON AGE, SEX (eg below)\n",
        "    Age       Female      Male\n",
        "0–30 days\t13.4–19.9\t13.4–19.9\n",
        "31–60 days\t10.7–17.1\t10.7–17.1\n",
        "2–3 months\t9.0–14.1\t9.0–14.1\n",
        "3–6 months\t9.5–14.1\t9.5–14.1\n",
        "6–12 months\t11.3–14.1\t11.3–14.1\n",
        "1–5 years\t10.9–15.0\t10.9–15.0\n",
        "5–11 years\t11.9–15.0\t11.9–15.0\n",
        "11–18 years\t11.9–15.0\t12.7–17.7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekNdQMLvuPm5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##############################################################\n",
        "## Comment out if running on google colab                    #\n",
        "## Remember to replace the file ID accordingly if using colab#\n",
        "##############################################################\n",
        "\n",
        "# # Import PyDrive and associated libraries.\n",
        "# # This only needs to be done once per notebook.\n",
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# # Authenticate and create the PyDrive client.\n",
        "# # This only needs to be done once per notebook.\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "\n",
        "# # Download a file based on its file ID.\n",
        "# # A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "# cpUP_file = '19un53B6ZHQnAeI74T3Swb85C-U5Sx4DE'\n",
        "# cpUK_file = '1Q5U4BieMs5pRf0kNNaErktL4oT3zPVC0'\n",
        "# cpOR_file = '1KwM5NDsyIFvpX0SwmEUcm3CtU6nXFZfZ'\n",
        "# cpMP_file = '1xs30vLQ9M41Seyx6Xi6f3jeXvSezXvA1'\n",
        "# cpJH_file = '1xXn0MXiRDL867Q1DPe4nWZR7Du_UnhIx'\n",
        "# cpCH_file = '1086hY7rzlbnz_VSOD7Vr1pY06330PMUR'\n",
        "# cpBH_file = '1JGFd98ZsfS0vQE8HBneMyWa57a0ToAWF'\n",
        "# bd_file = '1HPPyENxszY_vNHoQ2kjVLs_VPKt7_BPE'\n",
        "# ht_file = '1FU7WmHTWi7sMUbrqXIX_TBqND0w-TZ7h'\n",
        "\n",
        "# dcpUP_file = drive.CreateFile({'id': cpUP_file})\n",
        "# dcpUK_file = drive.CreateFile({'id': cpUK_file})\n",
        "# dcpOR_file = drive.CreateFile({'id': cpOR_file})\n",
        "# dcpMP_file = drive.CreateFile({'id': cpMP_file})\n",
        "# dcpJH_file = drive.CreateFile({'id': cpJH_file})\n",
        "# dcpBH_file = drive.CreateFile({'id': cpBH_file})\n",
        "# dcpCH_file = drive.CreateFile({'id': cpCH_file})\n",
        "# dbd_file =  drive.CreateFile({'id': bd_file})\n",
        "# dht_file =  drive.CreateFile({'id': ht_file})\n",
        "\n",
        "# dcpUP_file.GetContentFile('preprocessedUttarPradesh.csv')  \n",
        "# dcpUK_file.GetContentFile('preprocessedUttarakhand.csv')  \n",
        "# dcpOR_file.GetContentFile('preprocessedOrissa.csv')  \n",
        "# dcpMP_file.GetContentFile('preprocessedMadhyaPradesh.csv')  \n",
        "# dcpJH_file.GetContentFile('preprocessedJharkhand.csv')  \n",
        "# dcpCH_file.GetContentFile('preprocessedChattisgarh.csv')  \n",
        "# dcpBH_file.GetContentFile('preprocessedBihar.csv')\n",
        "# dbd_file.GetContentFile('binarized_data.csv')\n",
        "# dht_file.GetContentFile('hash_table_suppcnt.pkl')\n",
        "\n",
        "import pandas as pd\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qxGAybj5xkp",
        "colab_type": "text"
      },
      "source": [
        "Calculate Lift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU7buoG-5xyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lift(x_itemset,y_itemset,itemset,hash_table,trans_cnt):\n",
        "\n",
        "    lift_value = (hash_table[itemset] * trans_cnt)/(hash_table[x_itemset] * hash_table[y_itemset])\n",
        "\n",
        "    return lift_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PLZsnKKcURg",
        "colab_type": "text"
      },
      "source": [
        "Calculate Confidence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lombEsk5cUfr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confidence(x_itemset,itemset,hash_table):\n",
        "\n",
        "    confidence_value = hash_table[itemset]/hash_table[x_itemset]\n",
        "\n",
        "    return confidence_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC4AqbYg4WWP",
        "colab_type": "text"
      },
      "source": [
        "Association Rule Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WzSXumJAdW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from itertools import chain, combinations\n",
        "\n",
        "def generate_assoc_rules(freq_itemsets_list,hash_table,trans_cnt,min_conf,min_lift):\n",
        "\n",
        "    #  data structure to store possible association rules efficiently\n",
        "    assoc_rules = dict()\n",
        "\n",
        "    for itemset_list in freq_itemsets_list:\n",
        "        itemset = frozenset(itemset_list)\n",
        "        # Entering rules X-->Y in dictionary with key as X and Y as value\n",
        "        for x_itemset in list(chain.from_iterable(combinations(itemset_list,r) for r in range(1,len(itemset_list)))):\n",
        "            # Frozenset removes the necessity of taking care of order of the input items in a list\n",
        "            x_itemset = frozenset(x_itemset)\n",
        "            y_itemset = itemset.difference(x_itemset);\n",
        "            # Checking whether rules satisfy min confidence and min lift criteria\n",
        "            if confidence(x_itemset,itemset,hash_table) >= min_conf and lift(x_itemset,y_itemset,itemset,hash_table,trans_cnt) >= min_lift:\n",
        "                if x_itemset in assoc_rules:\n",
        "                    assoc_rules[x_itemset].append(y_itemset)\n",
        "                else :\n",
        "                    assoc_rules[x_itemset] = [y_itemset]\n",
        "\n",
        "    return assoc_rules\n",
        "\n",
        "# test = [[1,2,5],[3,4],[1,4,6]]\n",
        "# generate_assoc_rules(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UbxGYxwnvdH",
        "colab_type": "text"
      },
      "source": [
        "A method which takes two sorted itemsets and returns a merged and sorted itemset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRR5irEMnvdJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mergeItemSets(itemSet1, itemSet2, itemList):\n",
        "    \n",
        "    #Sorted list of items in first itemset\n",
        "    itemSortedList1=[]\n",
        "    #Sorted list of items in second itemset\n",
        "    itemSortedList2=[]\n",
        "    #Merged sorted list of items \n",
        "    mergedList=[]\n",
        "    \n",
        "    #Constructing corresponding sorted list of item numbers for items in first itemset \n",
        "    for item in itemSet1:\n",
        "        itemSortedList1.append(itemList.index(item))\n",
        "     \n",
        "    #Constructing corresponding sorted list of item numbers for items in first itemset\n",
        "    for item in itemSet2:\n",
        "        itemSortedList2.append(itemList.index(item))\n",
        "    \n",
        "    i=0\n",
        "    j=0\n",
        "    \n",
        "    #Merging the two sorted lists as in Merge Sort\n",
        "    while i<len(itemSortedList1) and j<len(itemSortedList2):\n",
        "        if itemSortedList1[i]<itemSortedList2[j]:\n",
        "            mergedList.append(itemSortedList1[i])\n",
        "            i=i+1\n",
        "        elif itemSortedList1[i]>itemSortedList2[j]:\n",
        "            mergedList.append(itemSortedList2[j])\n",
        "            j=j+1\n",
        "        else:\n",
        "            mergedList.append(itemSortedList1[i])\n",
        "            mergedList.append(itemSortedList2[j])\n",
        "            i=i+1\n",
        "            j=j+1\n",
        "            \n",
        "    while i<len(itemSortedList1):\n",
        "        mergedList.append(itemSortedList1[i])\n",
        "        i=i+1\n",
        "        \n",
        "    while j<len(itemSortedList2):\n",
        "        mergedList.append(itemSortedList2[j])\n",
        "        j=j+1\n",
        "    \n",
        "    #Reverting the sorted list of item numbers to a sorted list of items\n",
        "    for i in range(0, len(mergedList)):\n",
        "        mergedList[i]=itemList[mergedList[i]]\n",
        "        \n",
        "    return mergedList"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IkDX6mqNaPu",
        "colab_type": "text"
      },
      "source": [
        "Calculate Support Count"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WD4KdZ9DNae1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def supp_count(hash_table,itemset_list):\n",
        "\n",
        "    itemset = frozenset(itemset_list)\n",
        "\n",
        "    if itemset in hash_table :\n",
        "        return hash_table[itemset]\n",
        "\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CrNlTd0nvdk",
        "colab_type": "text"
      },
      "source": [
        "A method which takes two lists and checks if they share a prefix of a given length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdE9STzvnvdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def haveCommonPrefix(list1, list2, prefixLength):\n",
        "    \n",
        "    answer=True\n",
        "    \n",
        "    for i in range(0, prefixLength):\n",
        "        if list2[i]!=list1[i]:\n",
        "            answer=False\n",
        "            break\n",
        "            \n",
        "    return answer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LkQrTrLnvdw",
        "colab_type": "text"
      },
      "source": [
        "A method which takes a list of itemsets and returns it after eliminating all infrequent itemsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTbkvR76nvdx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def elimInfreqCandItemSets(prunedCandItemSets, minSup, hashTable):\n",
        "    \n",
        "    #List of all frequent K itemsets \n",
        "    KItemSets=[]\n",
        "    \n",
        "    #Eliminating each infrequent K-itemset\n",
        "    for itemSet in prunedCandItemSets:\n",
        "        supCount=supp_count(hashTable, itemSet)\n",
        "        if supCount>=minSup:\n",
        "            KItemSets.append(itemSet)\n",
        "             \n",
        "    return KItemSets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAL1K8r3nvd7",
        "colab_type": "text"
      },
      "source": [
        "A method which generates all subsets of an itemset, having length one less than that of the itemset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH_HPp6Gnvd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "\n",
        "def generateSubsetsOfItemSet(itemSet):\n",
        "    return list(itertools.combinations(set(itemSet), len(itemSet)-1)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_LmUZATnveI",
        "colab_type": "text"
      },
      "source": [
        "A method which takes a list of itemsets and returns it after eliminating the itemsets having infrequent subsets, of size one less than that of the itemset that is, prunes the list of itemsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzkty5TgnveK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pruneCandItemSets(candItemSets, minSup, hashTable):\n",
        "    \n",
        "    #List of pruned candidate K-itemsets\n",
        "    prunedCandItemSets=[]\n",
        "    \n",
        "    #Pruning each cadidate itemset if it has atleast one subset (sized K-1) which is infrequent\n",
        "    for itemSet in candItemSets:\n",
        "        pruneItemSet=0\n",
        "        #Generating K-1 subsets of itemset\n",
        "        subsets=generateSubsetsOfItemSet(itemSet)\n",
        "        #Checking support count of each subset\n",
        "        for subset in subsets:\n",
        "            supCount=supp_count(hashTable, list(subset))\n",
        "            #Pruning itemset if subset is infrequent\n",
        "            if supCount<minSup:\n",
        "                pruneItemSet=1\n",
        "                break\n",
        "        if pruneItemSet==0:\n",
        "            prunedCandItemSets.append(itemSet)\n",
        "            \n",
        "    return prunedCandItemSets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NrN5Eu8nveW",
        "colab_type": "text"
      },
      "source": [
        "A method which generates candidate itemsets of a given size from the frequent itemsets of one lesser size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZUAB5FJnveX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateCandItemSets(K, prevSetOfFreqItemSets, itemList):\n",
        "    \n",
        "    #List of generated candidate K-itemsets\n",
        "    candItemSets=[]\n",
        "    \n",
        "    #Merging pairs of K-1 itemsets which share a K-2 prefix\n",
        "    for itemSet1Index in range(0, len(prevSetOfFreqItemSets)):\n",
        "        for itemSet2Index in range(itemSet1Index+1, len(prevSetOfFreqItemSets)):\n",
        "            #For K=2 \n",
        "            if K==2:\n",
        "                candItemSets.append(mergeItemSets(prevSetOfFreqItemSets[itemSet1Index], prevSetOfFreqItemSets[itemSet2Index], itemList))\n",
        "            #For K>2 check for common K-2 prefix\n",
        "            elif haveCommonPrefix(prevSetOfFreqItemSets[itemSet1Index], prevSetOfFreqItemSets[itemSet2Index], K-2):\n",
        "                prefix=prevSetOfFreqItemSets[itemSet1Index][:K-2]\n",
        "                candItemSets.append(prefix+mergeItemSets(prevSetOfFreqItemSets[itemSet1Index][K-2:], prevSetOfFreqItemSets[itemSet2Index][K-2:], itemList))\n",
        "            \n",
        "    return candItemSets        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwqmO78Ynvek",
        "colab_type": "text"
      },
      "source": [
        "A method which generates frequent itemsets of a given size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fquz4I4Knveo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateFreqKItemSets(K, minSup, prevSetOfFreqItemSets, itemList, hashTable):\n",
        "    \n",
        "    #List of K-itemsets for given minSup\n",
        "    freqKItemSets=[]\n",
        "    \n",
        "    #Generating frequent K-itemsets\n",
        "    \n",
        "    #For K=1\n",
        "    if K==1:\n",
        "        for attr in itemList:\n",
        "            supCount=supp_count(hashTable, [attr])\n",
        "            if supCount>=minSup:\n",
        "                freqKItemSets.append([attr])\n",
        "    #For K>1\n",
        "    else:\n",
        "        #Candidate generation\n",
        "        candItemSets=generateCandItemSets(K, prevSetOfFreqItemSets, itemList)\n",
        "        #Candidate pruning\n",
        "        prunedCandItemSets=pruneCandItemSets(candItemSets, minSup, hashTable)\n",
        "        #Candidate elimination\n",
        "        freqKItemSets=elimInfreqCandItemSets(prunedCandItemSets, minSup, hashTable)\n",
        "        \n",
        "    return freqKItemSets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J55hj0Henve0",
        "colab_type": "text"
      },
      "source": [
        "A method which generates frequent itemsets of all sizes for a given list of itemsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li6p81bSnve2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generateFreqItemSets(itemList, minSup, hashTable):\n",
        "    \n",
        "    #Maximum size of frequent itemset which can be generated\n",
        "    maxSizeOfItemSet=len(itemList)\n",
        "    #List of all frequent itemsets for given minSup\n",
        "    freqItemSets=[]\n",
        "    #List of all K-itemsets for given minSup\n",
        "    KItemSets=[]\n",
        "    \n",
        "    #Generating frequent K-itemsets\n",
        "    for K in range(1, maxSizeOfItemSet):\n",
        "        KItemSets=generateFreqKItemSets(K, minSup, KItemSets, itemList, hashTable)\n",
        "        freqItemSets = freqItemSets + KItemSets\n",
        "        #Stopping criterion\n",
        "        if len(KItemSets)<=1:\n",
        "            break\n",
        "        \n",
        "    return freqItemSets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOdPRbtTnvfB",
        "colab_type": "text"
      },
      "source": [
        "A driver methods which uses the methods above to generate frequent itemsets and corresponding association rules for all minimum support values given to it as input "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JxNRohxnvfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "def performAssocRuleMining_freqItemsets(itemList, minSupFractions, numOfTransac, hashTable):\n",
        "    \n",
        "    #Generating frequent itemsets for each minSup value\n",
        "    for minSupFraction in minSupFractions:\n",
        "        minSup=math.ceil(minSupFraction*numOfTransac)\n",
        "        freqItemSets=generateFreqItemSets(itemList, minSup, hashTable)\n",
        "        \n",
        "        fn = \"../Data/Association Rule Mining Data/Frequent Itemsets/freq_itemsets_ms\"+str(minSupFraction)+\".pkl\"\n",
        "        wf = open(fn,\"wb\")\n",
        "        pickle.dump(freqItemSets, wf)\n",
        "        wf.close()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTwwZdOaNuf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def performAssocRuleMining_assocRules(freqItemSets,hashTable,transCnt,min_conf,min_lift):\n",
        "\n",
        "    #List of association rules for all minimum support values put together\n",
        "    assocRulesForAllMinSup=[]\n",
        "    \n",
        "    # Generate association rules for minSup value\n",
        "    assocRulesForAllMinSup.append(generate_assoc_rules(freqItemSets, hashTable,transCnt, min_conf,min_lift)) \n",
        "\n",
        "    return assocRulesForAllMinSup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h2qQEyUOd3v",
        "colab_type": "text"
      },
      "source": [
        "Hash Table Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9jY9jj6OeFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from itertools import chain,combinations\n",
        "\n",
        "def generate_hash_table(df):\n",
        "\n",
        "    hash_table = dict()\n",
        "    row_count,col_count = df.shape\n",
        "    columns = [col for col in df.columns]\n",
        "\n",
        "    for row_no in range(row_count):\n",
        "        # Making the list of all the columns having value 1 for a data record.\n",
        "        transaction  = [columns[col_no] for col_no in range(col_count) if df.iloc[row_no][col_no]==1]\n",
        "        # Generating all subsets of the columns having value 1 for a data record.\n",
        "        for itemset in list(chain.from_iterable(combinations(transaction,r) for r in range(1,len(transaction)+1))):\n",
        "            itemset = frozenset(itemset)\n",
        "            # Check if item is already in the hash table else add it.\n",
        "            if itemset in hash_table:\n",
        "                hash_table[itemset] += 1\n",
        "            else :\n",
        "                hash_table[itemset] = 1\n",
        "\n",
        "    return hash_table\n",
        "\n",
        "# test = {'a':[0,1],'b':[1,0],'c':[1,1],'d':[1,1]}\n",
        "# df = pd.DataFrame.from_dict(test)\n",
        "# hash_table = generate_hash_table(df)\n",
        "# print(supp_count(hash_table,['x','p']))\n",
        "# generate_assoc_rules([['x','z','p']],hash_table,0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIrtTaXrnvfb",
        "colab_type": "text"
      },
      "source": [
        "A method which handles categorical attributes of the given dataframe by binarizing them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLDLBfbenvfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def handleCategoricalAttr(categoricalAttr, dataFrame):\n",
        "    \n",
        "    #Binarization of each categorical attribute\n",
        "    for attr in categoricalAttr:\n",
        "        dataFrame=pd.get_dummies(dataFrame, columns=[attr], prefix=[attr])\n",
        "        \n",
        "    return dataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYoxQpzonvfp",
        "colab_type": "text"
      },
      "source": [
        "A method which handles continuous attributes of the given dataframe by discretizing them according to the given intervals and subsequently binarizing them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_xjhzO1nvfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def handleContinuousAttr(contAttr, bins, dataFrame):\n",
        "    \n",
        "    #Discretization of each continuous attribute\n",
        "    for i in range(0, len(contAttr)):\n",
        "        dataFrame[contAttr[i]]=pd.cut(dataFrame[contAttr[i]], bins[i], right=False, labels=list(str(num) for num in range(1,len(bins[i]))))\n",
        "        \n",
        "    #Binarization of each discretized attribute\n",
        "    dataFrame=handleCategoricalAttr(contAttr, dataFrame)\n",
        "    \n",
        "    return dataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVA1nNglqMMQ",
        "colab_type": "text"
      },
      "source": [
        "A driver method which uses the above methods to handle categorical and continuous attributes of a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TR3gtnLQnvgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def handleAttr(dataFrame):\n",
        "    \n",
        "    #List of categorical attributes\n",
        "    categoricalAttr=['state_code', 'rural_urban', 'Sex']\n",
        "    #List of continuous attributes\n",
        "    contAttr=['Age', 'BMI', 'test_salt_iodine', 'Haemoglobin_level', 'BP_systolic', 'BP_Diastolic', 'Pulse_rate', 'fasting_blood_glucose_mg_dl']\n",
        "    #List of intervals for discretization of continuous attributes\n",
        "    bins=[[1, 20, 40, 80, 100], [1, 18.5, 25, 30, 100], [0, 15, 31], [1, 9, 10, 11, 12, 13, 20], [0, 90, 120, 130, 140, 181, 200], [0, 60, 80, 90, 121, 150], [0, 40, 55, 60, 75, 80, 150], [50, 70, 100, 126, 350]]#list of lists filled in with domain knowledge\n",
        "    \n",
        "    #Handling categorical attributes\n",
        "    dataFrame=handleCategoricalAttr(categoricalAttr, dataFrame)\n",
        "    #Handling continuous attributes\n",
        "    dataFrame=handleContinuousAttr(contAttr, bins, dataFrame)\n",
        "    \n",
        "    return dataFrame"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "204qFtUamsPh",
        "colab_type": "text"
      },
      "source": [
        "Preparing for Association Rules Mining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4BztRaHnvgT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initiating_ARM():\n",
        "    \n",
        "    #Defining list containing the names of all states\n",
        "    stateNames=[\"Bihar\", \"Chattisgarh\", \"Jharkhand\", \"MadhyaPradesh\", \"Orissa\", \"Uttarakhand\", \"UttarPradesh\"]\n",
        "    \n",
        "    #Defining the file path prefix\n",
        "    pathPrefix=\"../Data/Preprocessed Data/preprocessed\"\n",
        "    \n",
        "    #Creating an empty dataframe\n",
        "    dataFrame=pd.DataFrame()\n",
        "\n",
        "    #Concatenating preprocessed data from all states\n",
        "    for state in stateNames:\n",
        "        dataFrame=dataFrame.append(pd.read_csv(pathPrefix+state+'.csv', low_memory=False), ignore_index=True)\n",
        "        \n",
        "    #Keeping only the attributes relevant for association rule mining\n",
        "    dataFrame=pd.DataFrame(dataFrame, columns=['state_code', 'rural_urban', 'Sex', 'Age', 'BMI', 'test_salt_iodine', 'Haemoglobin_level', 'BP_systolic', 'BP_Diastolic', 'Pulse_rate', 'fasting_blood_glucose_mg_dl'])\n",
        "    \n",
        "    #Handling categorical and continuous attributes\n",
        "    dataFrame=handleAttr(dataFrame)\n",
        "        \n",
        "    #Generating the hashtable for support of each itemset\n",
        "    hashTable=generate_hash_table(dataFrame)\n",
        "\n",
        "    #Store both hash table and binarized data to avoid re-executions\n",
        "    dataFrame.to_csv('../Data/Association Rule Mining Data/binarized_data.csv',index=False)\n",
        "\n",
        "    wf = open(\"../Data/Association Rule Mining Data/hash_table_suppcnt.pkl\",\"wb\")\n",
        "    pickle.dump(hashTable,wf)\n",
        "    wf.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trVtQZsXmy-H",
        "colab_type": "text"
      },
      "source": [
        "Driver functions to perform Association Rules Mining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZSdqJC8g1Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def execute_ARM_genFreqItemsets():\n",
        "\n",
        "    # Reading binarized data in dataframe\n",
        "    dataFrame = pd.read_csv('../Data/Association Rule Mining Data/binarized_data.csv')\n",
        "    \n",
        "    # Reading hash table containing support count from csv\n",
        "    hashTable = pickle.load(open(\"../Data/Association Rule Mining Data/hash_table_suppcnt.pkl\", \"rb\"))\n",
        "    \n",
        "    # Creating the list of 'items' that is, the list of columns in the dataframe\n",
        "    itemList=[]\n",
        "    for colIndex in range(0, len(dataFrame.columns)):\n",
        "        itemList.append(dataFrame.columns[colIndex])\n",
        "    \n",
        "    # List of minSup values that the user wants to experiment with (in fractions)\n",
        "    minSupFractions=[0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "    \n",
        "    #Generating itemsets for all minSup values\n",
        "    performAssocRuleMining_freqItemsets(itemList, minSupFractions, dataFrame.shape[0], hashTable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rur0KiiVQmg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "def execute_ARM_genAssocRules():\n",
        "\n",
        "    # All possible value of min support, min confidence and min lift values\n",
        "    minSupFraction=[0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "    minConfFraction=[0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "    minLiftFraction=[0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "\n",
        "    # Reading binarized data in dataframe\n",
        "    dataFrame = pd.read_csv('../Data/Association Rule Mining Data/binarized_data.csv')\n",
        "    transCnt = dataFrame.shape[0]\n",
        "\n",
        "    # Reading hash table containing support count from csv\n",
        "    hashTable = pickle.load(open(\"../Data/Association Rule Mining Data/hash_table_suppcnt.pkl\", \"rb\"))\n",
        "\n",
        "    for minSup in minSupFraction:\n",
        "\n",
        "        fn = \"../Data/Association Rule Mining Data/Frequent Itemsets/freq_itemsets_ms\"+str(minSup)+\".pkl\"\n",
        "        freqItemsets = pickle.load(open(fn,\"rb\"))\n",
        "\n",
        "        for minConf in minConfFraction: \n",
        "            for minLift in minLiftFraction: \n",
        "\n",
        "                #Generating association rules for minSup,min values\n",
        "                assocRules=performAssocRuleMining_assocRules(freqItemsets, hashTable, transCnt,minConf, minLift)\n",
        "\n",
        "                fn = '../Data/Association Rule Mining Data/Association_Rules/assoc_rules_ms('+str(minSup)+')_mc('+str(minConf)+')_ml('+str(minLift)+').txt'\n",
        "                rules_file = open(fn, 'w') \n",
        "                \n",
        "                #Printing association rules for all minSup values\n",
        "                print('Association Rules:')\n",
        "                rulecnt = 0\n",
        "                print(\"Min Sup Fraction = \"+str(minSup)+\"%\\n\",file = rules_file)\n",
        "                print(\"Min Conf Fraction= \"+str(minConf)+\"%\\n\",file = rules_file)\n",
        "                print(\"Min Lift Fraction = \"+str(minLift)+\"%\\n\",file = rules_file)\n",
        "                for key,value in assocRules[0].items():\n",
        "                    rulecnt = rulecnt + len(value)\n",
        "                    for y in value:\n",
        "                        print(str(set(key))+\" --> \"+str(set(y)),file = rules_file)\n",
        "\n",
        "                print(rulecnt)\n",
        "\n",
        "                rules_file.close()\n",
        "\n",
        "                # remove the files with no association rules in them \n",
        "                if rulecnt == 0 :\n",
        "                    os.remove(fn)\n",
        "                    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIIHlgi2qILK",
        "colab_type": "text"
      },
      "source": [
        "Main functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saI1iqHOnvgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To avoid unecessary request to create support hash table and binarized data\n",
        "# Need to be run only when running for first time and dont have binarized data, hash_table file\n",
        "initiating_ARM()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkjFobhuii0K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main driver program to generate frequent itemsets\n",
        "# Need to be run only when running for first time and you dont have min_support files \n",
        "execute_ARM_genFreqItemsets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHSgG-1PQ70_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Main driver program to generate association rules\n",
        "execute_ARM_genAssocRules()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}